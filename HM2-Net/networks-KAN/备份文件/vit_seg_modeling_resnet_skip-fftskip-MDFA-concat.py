import math

from os.path import join as pjoin
from collections import OrderedDict

import torch
import torch.nn as nn
import torch.nn.functional as F
from .FADC import OmniAttention, FrequencySelection, generate_laplacian_pyramid, AdaptiveDilatedConv
from  .FADConv import AdaptiveDilatedConv1


class MultiDimensionalFrequencyAttention(nn.Module):
    """
    å¤šç»´é¢‘ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMDFAï¼‰+ æ®‹å·®è¿æ¥
    1. **é€šé“æ³¨æ„åŠ›**: `OmniAttention` è®¡ç®—ä¸åŒé€šé“çš„é‡è¦æ€§
    2. **ç©ºé—´æ³¨æ„åŠ›**: `OmniAttention` è®¡ç®—ç©ºé—´åŠ æƒ
    3. **é¢‘ç‡æ³¨æ„åŠ›**: `FrequencySelection` è®¡ç®—ä¸åŒé¢‘ç‡æˆåˆ†çš„åŠ æƒ
    4. **é«˜ä½é¢‘ç‰¹å¾åˆ†è§£**: `generate_laplacian_pyramid()` è¿›è¡Œé«˜ä½é¢‘ä¿¡æ¯åˆ†è§£
    5. **é¢‘ç‡æ©ç **: `AdaptiveDilatedConv.freq_select()` è¿›è¡Œé«˜ä½é¢‘æ©ç å¢å¼º
    6. **æ®‹å·®è¿æ¥**: ç›´æ¥åŠ å…¥ `x`ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
    """

    def __init__(self, in_channels, kernel_size=3, k_list=[2, 4, 8], lp_type='freq', use_residual=True):
        super(MultiDimensionalFrequencyAttention, self).__init__()
        self.in_channels = in_channels
        self.use_residual = use_residual  # æ§åˆ¶æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥

        # **é€šé“ & ç©ºé—´æ³¨æ„åŠ›**
        self.omni_attention = OmniAttention(in_channels, in_channels, kernel_size)

        # **é¢‘ç‡æ³¨æ„åŠ›**
        self.frequency_attention = FrequencySelection(in_channels, k_list=k_list, lp_type=lp_type)

        # **é«˜ä½é¢‘åˆ†è§£**
        self.laplacian_pyramid_levels = len(k_list)

        # **é¢‘ç‡æ©ç **
        self.freq_selector = AdaptiveDilatedConv(in_channels, in_channels, kernel_size=3)

    def forward(self, x):
        B, C, H, W = x.shape
        residual = x  # **æ®‹å·®è¿æ¥**

        # **1. è®¡ç®— FFT é¢‘è°±**
        fft_x = torch.fft.fft2(x, norm='ortho')
        fft_x_real = fft_x.real
        fft_x_imag = fft_x.imag

        # **2. è®¡ç®—é€šé“æ³¨æ„åŠ›**
        channel_att = self.omni_attention.get_channel_attention(fft_x_real)

        # **ğŸš€ è¿™é‡Œä¿®æ­£ `channel_att` çš„è®¡ç®—æ–¹å¼**
        if channel_att.shape[1] != C:
            # print(f"[DEBUG] ä¿®æ­£ channel_att å½¢çŠ¶: {channel_att.shape} -> {(B, C, 1, 1)}")
            channel_att = channel_att.view(B, C, H, W)  # é‡æ–°è°ƒæ•´å½¢çŠ¶
            channel_att = F.adaptive_avg_pool2d(channel_att, (1, 1))  # **ç¡®ä¿æ˜¯ (B, C, 1, 1)**

        # **3. æ–½åŠ é€šé“æ³¨æ„åŠ›**
        fft_x_real = fft_x_real * channel_att
        fft_x_imag = fft_x_imag * channel_att

        # **4. è®¡ç®—ç©ºé—´æ³¨æ„åŠ›**
        spatial_att = self.omni_attention.get_spatial_attention(fft_x_real)
        fft_x_real = fft_x_real * spatial_att
        fft_x_imag = fft_x_imag * spatial_att

        # **5. è®¡ç®—ä¸åŒé¢‘ç‡æ³¨æ„åŠ›**
        freq_weighted_x = self.frequency_attention(x)

        # **6. è®¡ç®—é«˜ä½é¢‘åˆ†è§£**
        pyramid = generate_laplacian_pyramid(x, self.laplacian_pyramid_levels)
        high_freq_part = sum(pyramid[:-1])  # é«˜é¢‘éƒ¨åˆ†
        low_freq_part = pyramid[-1]  # ä½é¢‘éƒ¨åˆ†

        # **7. è®¡ç®—é¢‘ç‡æ©ç **
        freq_mask = self.freq_selector.freq_select(x)

        # **8. ç»“åˆæ‰€æœ‰æ³¨æ„åŠ›**
        fft_x_real = fft_x_real + freq_weighted_x
        fft_x_imag = fft_x_imag + freq_weighted_x
        fft_x_real = fft_x_real * freq_mask
        fft_x_imag = fft_x_imag * freq_mask

        # **9. æ‰§è¡Œ IFFT è¿˜åŸæ—¶åŸŸ**
        fft_x_updated = torch.complex(fft_x_real, fft_x_imag)
        x_new = torch.fft.ifft2(fft_x_updated, norm='ortho').real

        # **10. åŠ å…¥é«˜ä½é¢‘ä¿¡æ¯**
        x_new = x_new + high_freq_part + low_freq_part

        # **11. æ®‹å·®è¿æ¥**
        if self.use_residual:
            x_new = x_new + residual  # ç›´æ¥åŠ ä¸Šè¾“å…¥ï¼Œå¢å¼ºç¨³å®šæ€§

        return x_new


def np2th(weights, conv=False):
    """Possibly convert HWIO to OIHW."""
    if conv:
        weights = weights.transpose([3, 2, 0, 1])
    return torch.from_numpy(weights)


class StdConv2d(nn.Conv2d):

    def forward(self, x):
        w = self.weight
        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)
        w = (w - m) / torch.sqrt(v + 1e-5)
        return F.conv2d(x, w, self.bias, self.stride, self.padding,
                        self.dilation, self.groups)


def conv3x3(cin, cout, stride=1, groups=1, bias=False):
    return StdConv2d(cin, cout, kernel_size=3, stride=stride,
                     padding=1, bias=bias, groups=groups)


def conv1x1(cin, cout, stride=1, bias=False):
    return StdConv2d(cin, cout, kernel_size=1, stride=stride,
                     padding=0, bias=bias)


class PreActBottleneck(nn.Module):
    """Pre-activation (v2) bottleneck block with AdaptiveDilatedConv1."""

    def __init__(self, cin, cout=None, cmid=None, stride=1, dilation=1):
        super().__init__()
        cout = cout or cin
        cmid = cmid or cout // 4

        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)
        self.conv1 = conv1x1(cin, cmid, bias=False)
        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)
        # ä½¿ç”¨ AdaptiveDilatedConv1 æ›¿ä»£ conv3x3
        self.conv2 = AdaptiveDilatedConv1(cmid, cmid, kernel_size=3, stride=stride, dilation=dilation, bias=False)
        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)
        self.conv3 = conv1x1(cmid, cout, bias=False)
        self.relu = nn.ReLU(inplace=True)

        if (stride != 1 or cin != cout):
            self.downsample = conv1x1(cin, cout, stride, bias=False)
            self.gn_proj = nn.GroupNorm(cout, cout)

    def forward(self, x):
        # Residual branch
        residual = x
        if hasattr(self, 'downsample'):
            residual = self.downsample(x)
            residual = self.gn_proj(residual)

        # Unit's branch
        y = self.relu(self.gn1(self.conv1(x)))
        y = self.relu(self.gn2(self.conv2(y)))  # ä½¿ç”¨ AdaptiveDilatedConv1
        y = self.gn3(self.conv3(y))

        y = self.relu(residual + y)
        return y

    def load_from(self, weights, n_block, n_unit):
        conv1_weight = np2th(weights[pjoin(n_block, n_unit, "conv1/kernel")], conv=True)
        conv2_weight = np2th(weights[pjoin(n_block, n_unit, "conv2/kernel")], conv=True)
        conv3_weight = np2th(weights[pjoin(n_block, n_unit, "conv3/kernel")], conv=True)

        gn1_weight = np2th(weights[pjoin(n_block, n_unit, "gn1/scale")])
        gn1_bias = np2th(weights[pjoin(n_block, n_unit, "gn1/bias")])

        gn2_weight = np2th(weights[pjoin(n_block, n_unit, "gn2/scale")])
        gn2_bias = np2th(weights[pjoin(n_block, n_unit, "gn2/bias")])

        gn3_weight = np2th(weights[pjoin(n_block, n_unit, "gn3/scale")])
        gn3_bias = np2th(weights[pjoin(n_block, n_unit, "gn3/bias")])

        self.conv1.weight.copy_(conv1_weight)
        self.conv2.weight.copy_(conv2_weight)
        self.conv3.weight.copy_(conv3_weight)

        self.gn1.weight.copy_(gn1_weight.view(-1))
        self.gn1.bias.copy_(gn1_bias.view(-1))

        self.gn2.weight.copy_(gn2_weight.view(-1))
        self.gn2.bias.copy_(gn2_bias.view(-1))

        self.gn3.weight.copy_(gn3_weight.view(-1))
        self.gn3.bias.copy_(gn3_bias.view(-1))

        if hasattr(self, 'downsample'):
            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, "conv_proj/kernel")], conv=True)
            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, "gn_proj/scale")])
            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, "gn_proj/bias")])

            self.downsample.weight.copy_(proj_conv_weight)
            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))
            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))

class ResNetV2(nn.Module):
    """Implementation of Pre-activation (v2) ResNet model with MDFA at each stage."""

    def __init__(self, block_units, width_factor, dilation=1):
        super().__init__()
        width = int(64 * width_factor)
        self.width = width

        self.root = nn.Sequential(OrderedDict([
            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),
            ('gn', nn.GroupNorm(32, width, eps=1e-6)),
            ('relu', nn.ReLU(inplace=True)),
        ]))

        self.body = nn.Sequential(OrderedDict([
            ('block1', nn.Sequential(OrderedDict(
                [('unit1', PreActBottleneck(cin=width, cout=width * 4, cmid=width, dilation=dilation))] +
                [(f'unit{i:d}', PreActBottleneck(cin=width * 4, cout=width * 4, cmid=width, dilation=dilation)) for i in range(2, block_units[0] + 1)],
            ))),
            ('block2', nn.Sequential(OrderedDict(
                [('unit1', PreActBottleneck(cin=width * 4, cout=width * 8, cmid=width * 2, stride=2, dilation=dilation))] +
                [(f'unit{i:d}', PreActBottleneck(cin=width * 8, cout=width * 8, cmid=width * 2, dilation=dilation)) for i in range(2, block_units[1] + 1)],
            ))),
            ('block3', nn.Sequential(OrderedDict(
                [('unit1', PreActBottleneck(cin=width * 8, cout=width * 16, cmid=width * 4, stride=2, dilation=dilation))] +
                [(f'unit{i:d}', PreActBottleneck(cin=width * 16, cout=width * 16, cmid=width * 4, dilation=dilation)) for i in range(2, block_units[2] + 1)],
            ))),
        ]))

        # âœ… **åœ¨æ¯ä¸ª Stage ä¹‹åæ·»åŠ  MDFA**
        self.attn_mdfa0 = MultiDimensionalFrequencyAttention(width)
        self.attn_mdfa1 = MultiDimensionalFrequencyAttention(width * 4)
        self.attn_mdfa2 = MultiDimensionalFrequencyAttention(width * 8)
        self.attn_mdfa3 = MultiDimensionalFrequencyAttention(width * 16)

    def forward(self, x):
        features = []
        b, c, in_size, _ = x.size()

        # âœ… **Root å±‚å¤„ç†**
        x = self.root(x)  # ç»è¿‡ Root å±‚
        features.append(x)  # ğŸš€ æŠŠ `root` ä½œä¸ºç¬¬ä¸€ä¸ªè·³è·ƒè¿æ¥

        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)

        # âœ… **éå† block1, block2, block3**
        for i in range(len(self.body)):
            x = self.body[i](x)  # **å…ˆç»è¿‡ block**
            x = getattr(self, f'attn_mdfa{i + 1}')(x)  # **MDFA ä½œç”¨åœ¨ `block` ä¹‹å**

            if i < 2:  # âœ… åªå­˜å‚¨ `block1` å’Œ `block2`
                features.append(x)  # **MDFA å¤„ç†åçš„ `x` ä½œä¸º Skip Connection**

        return x, features  # **è¿”å›æ­£å¸¸é¡ºåºçš„ `features`**




